{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ollama\n",
    "https://docs.ollama.com/\n",
    "Open source and provides multipl LLM models\n",
    "\n",
    "Ollama supports embedding models, making it possible to build retrieval augmented generation (RAG) applications that combine text prompts\n",
    "with existing documents or other data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_t/qbz54v_j4b5_pq3b314cj9940000gp/T/ipykernel_33236/2885145215.py:4: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"gemma:2b\")  ##by default it ues llama2.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OllamaEmbeddings(base_url='http://localhost:11434', model='gemma:2b', embed_instruction='passage: ', query_instruction='query: ', mirostat=None, mirostat_eta=None, mirostat_tau=None, num_ctx=None, num_gpu=None, num_thread=None, repeat_last_n=None, repeat_penalty=None, temperature=None, stop=None, tfs_z=None, top_k=None, top_p=None, show_progress=False, headers=None, model_kwargs=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# You should download the model locally using ollama CLI to use it for embedding\n",
    "embeddings = OllamaEmbeddings(model=\"gemma:2b\")  ##by default it ues llama2.\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1=embeddings.embed_documents(\n",
    "    [\n",
    "       \"Alpha is the first letter of Greek alphabet\",\n",
    "       \"Beta is the second letter of Greek alphabet\", \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(r1[0]) #Gives the dimension of the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.embed_query(\"What is the second letter of Greek alphabet \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Other Embedding Models\n",
    "### https://ollama.com/blog/embedding-models\n",
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\") # You should download the model :: mxbai-embed-large locally\n",
    "text = \"This is a test document.\"\n",
    "query_result = embeddings.embed_query(text)\n",
    "query_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_result) #Gives the vector dimension"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
